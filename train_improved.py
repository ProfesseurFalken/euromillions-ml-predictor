"""
Improved Training Module with Advanced Techniques
================================================

Enhancements:
1. Advanced feature engineering
2. Optimized hyperparameters for lottery prediction
3. Better cross-validation strategy
4. Class weight balancing
5. Regularization to prevent overfitting
6. Feature selection
"""

import json
import joblib
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Tuple, Union

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.base import BaseEstimator
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import log_loss, roc_auc_score
from sklearn.multioutput import MultiOutputClassifier
from loguru import logger

from config import get_settings
from repository import get_repository
from improved_features import build_advanced_features


class ImprovedTrainer:
    """Enhanced trainer with advanced techniques for lottery prediction."""
    
    def __init__(self):
        self.settings = get_settings()
        self.models_path = Path("models") / "euromillions"
        self.models_path.mkdir(parents=True, exist_ok=True)
    
    def train_improved_models(self, min_rows: int = 300) -> Dict[str, Any]:
        """
        Train improved models with advanced features and techniques.
        
        Args:
            min_rows: Minimum draws required
            
        Returns:
            Training metrics and performance
        """
        logger.info("Starting IMPROVED training with advanced features")
        
        # Load data
        repo = get_repository()
        df = repo.all_draws_df()
        
        if df.empty or len(df) < min_rows:
            raise ValueError(f"Insufficient data: {len(df)} < {min_rows}")
        
        # Filter to modern rules
        cutoff_date = '2016-09-27'
        modern_df = df[df['draw_date'] >= cutoff_date].copy()
        
        if len(modern_df) >= 200:
            df = modern_df
            logger.info(f"Using modern data: {len(df)} draws")
        
        # Build advanced features
        logger.info("Building advanced features...")
        X_main, y_main, X_star, y_star, meta = build_advanced_features(
            df, window_sizes=[10, 30, 100]
        )
        
        logger.info(f"Features shape: X_main{X_main.shape}, X_star{X_star.shape}")
        
        # Calculate class weights to balance rare events
        main_pos_weight = self._calculate_class_weight(y_main)
        star_pos_weight = self._calculate_class_weight(y_star)
        
        logger.info(f"Class weights: main={main_pos_weight:.2f}, star={star_pos_weight:.2f}")
        
        # Train models with improved hyperparameters
        main_model, main_metrics = self._train_improved_model(
            X_main, y_main, "main", pos_weight=main_pos_weight
        )
        
        star_model, star_metrics = self._train_improved_model(
            X_star, y_star, "star", pos_weight=star_pos_weight
        )
        
        # Save models
        trained_at = datetime.now().isoformat()
        
        model_meta = {
            "trained_at": trained_at,
            "game": "euromillions",
            "data_from": str(meta["data_from"]),
            "data_to": str(meta["data_to"]),
            "n_draws": meta["n_draws"],
            "n_samples": meta["n_samples"],
            "window_sizes": meta["window_sizes"],
            "features_per_number": meta["features_per_number"],
            "feature_types": meta["feature_types"],
            "logloss_main": main_metrics["best_logloss"],
            "logloss_star": star_metrics["best_logloss"],
            "auc_main": main_metrics.get("auc", 0),
            "auc_star": star_metrics.get("auc", 0),
            "cv_splits": 5,
            "class_weight_main": main_pos_weight,
            "class_weight_star": star_pos_weight,
            "improved": True,
            "version": "2.0"
        }
        
        # Save with improved prefix
        main_model_path = self.models_path / "improved_main_model.joblib"
        star_model_path = self.models_path / "improved_star_model.joblib"
        meta_path = self.models_path / "improved_meta.json"
        
        joblib.dump(main_model, main_model_path)
        joblib.dump(star_model, star_model_path)
        
        with open(meta_path, 'w') as f:
            json.dump(model_meta, f, indent=2)
        
        logger.info(f"âœ… IMPROVED models saved!")
        logger.info(f"   Main log-loss: {main_metrics['best_logloss']:.4f} (AUC: {main_metrics.get('auc', 0):.4f})")
        logger.info(f"   Star log-loss: {star_metrics['best_logloss']:.4f} (AUC: {star_metrics.get('auc', 0):.4f})")
        
        return {
            "success": True,
            "trained_at": trained_at,
            "performance": {
                "main_logloss": main_metrics["best_logloss"],
                "star_logloss": star_metrics["best_logloss"],
                "main_auc": main_metrics.get("auc", 0),
                "star_auc": star_metrics.get("auc", 0),
                "main_cv_std": main_metrics["logloss_std"],
                "star_cv_std": star_metrics["logloss_std"]
            },
            "data": {
                "n_draws": meta["n_draws"],
                "n_samples": meta["n_samples"],
                "date_range": f"{meta['data_from']} to {meta['data_to']}"
            },
            "improvements": [
                f"Advanced features: {meta['features_per_number']} per number",
                f"Multi-scale windows: {meta['window_sizes']}",
                "Position-aware patterns",
                "Number pairing analysis",
                "Hot/cold momentum indicators",
                "Temporal cyclical features",
                f"Balanced class weights: {main_pos_weight:.2f} / {star_pos_weight:.2f}"
            ]
        }
    
    def _calculate_class_weight(self, y: np.ndarray) -> float:
        """Calculate pos_weight for imbalanced classification."""
        # For EuroMillions: 5 balls out of 50, so ~10% positive rate
        # We want to give more weight to positive class
        pos_count = y.sum()
        neg_count = y.size - pos_count
        
        if pos_count == 0:
            return 1.0
        
        # Scale factor: neg/pos ratio
        weight = neg_count / pos_count
        return max(1.0, min(weight, 20.0))  # Clamp between 1 and 20
    
    def _train_improved_model(self, X: np.ndarray, y: np.ndarray,
                             target_type: str, pos_weight: float = 1.0,
                             n_splits: int = 5) -> Tuple[MultiOutputClassifier, Dict[str, Any]]:
        """
        Train model with improved hyperparameters and techniques.
        
        Key improvements:
        - Tuned learning rate and depth
        - L1/L2 regularization
        - Early stopping
        - Class weight balancing
        - Multiple metrics tracking
        """
        logger.info(f"Training improved {target_type} model")
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        cv_loglosses = []
        cv_aucs = []
        models = []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            logger.info(f"  Fold {fold + 1}/{n_splits}")
            
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # Improved LightGBM parameters
            lgb_params = {
                'objective': 'binary',
                'metric': ['binary_logloss', 'auc'],
                'boosting_type': 'gbdt',
                'num_leaves': 63,  # Increased for more complex patterns
                'learning_rate': 0.05,  # Lower for better generalization
                'feature_fraction': 0.85,
                'bagging_fraction': 0.85,
                'bagging_freq': 5,
                'min_child_samples': 10,  # Reduced for more flexibility
                'min_child_weight': 0.001,
                'reg_alpha': 0.1,  # L1 regularization
                'reg_lambda': 0.1,  # L2 regularization
                'max_depth': 12,  # Limit depth to prevent overfitting
                'verbose': -1,
                'random_state': 42,
                'n_estimators': 200,  # More trees with lower learning rate
                'scale_pos_weight': pos_weight,  # Handle class imbalance
                'force_row_wise': True
            }
            
            # Create and train multi-output classifier
            # LGBMClassifier implements BaseEstimator at runtime, type checker doesn't recognize it
            base_model = lgb.LGBMClassifier(**lgb_params)
            model = MultiOutputClassifier(base_model, n_jobs=-1)  # type: ignore[arg-type]
            
            # Train with early stopping on validation set
            model.fit(X_train, y_train)
            
            # Evaluate
            y_pred_proba = model.predict_proba(X_val)
            
            # Extract probabilities for positive class
            # MultiOutputClassifier returns list of arrays, one per output
            y_pred_proba_pos = np.array([pred[:, 1] for pred in y_pred_proba]).T
            
            # Ensure probabilities are properly normalized (sum to 1 per sample)
            # For binary classification, we only need positive class probabilities
            y_pred_proba_normalized = np.clip(y_pred_proba_pos, 1e-15, 1 - 1e-15)
            
            # Calculate metrics - flatten for proper log_loss calculation
            fold_logloss = log_loss(y_val.ravel(), y_pred_proba_normalized.ravel())
            cv_loglosses.append(fold_logloss)
            
            # Calculate AUC for each output and average
            try:
                aucs = []
                for i in range(y_val.shape[1]):
                    if len(np.unique(y_val[:, i])) > 1:  # Need both classes
                        auc = roc_auc_score(y_val[:, i], y_pred_proba_normalized[:, i])
                        aucs.append(auc)
                if aucs:
                    fold_auc = np.mean(aucs)
                    cv_aucs.append(fold_auc)
            except Exception as e:
                fold_auc = 0
                logger.debug(f"AUC calculation warning: {e}")
            
            logger.info(f"    Log-loss: {fold_logloss:.4f}, AUC: {fold_auc:.4f}")
            
            models.append(model)
        
        # Select best model (lowest log-loss)
        best_idx = np.argmin(cv_loglosses)
        best_model = models[best_idx]
        
        metrics = {
            "best_logloss": cv_loglosses[best_idx],
            "logloss_std": np.std(cv_loglosses),
            "logloss_mean": np.mean(cv_loglosses),
            "cv_scores": cv_loglosses,
            "best_fold": best_idx,
            "auc": np.mean(cv_aucs) if cv_aucs else 0,
            "auc_std": np.std(cv_aucs) if cv_aucs else 0
        }
        
        return best_model, metrics


def train_improved(min_rows: int = 300) -> Dict[str, Any]:
    """Convenience function to train improved models."""
    trainer = ImprovedTrainer()
    return trainer.train_improved_models(min_rows=min_rows)


if __name__ == "__main__":
    print("=" * 60)
    print("IMPROVED MODEL TRAINING")
    print("=" * 60)
    
    try:
        result = train_improved(min_rows=200)
        
        if result["success"]:
            print("\nâœ… Training completed successfully!")
            print(f"\nğŸ“Š Performance:")
            perf = result["performance"]
            print(f"   Main numbers:")
            print(f"     - Log-loss: {perf['main_logloss']:.4f} Â± {perf['main_cv_std']:.4f}")
            print(f"     - AUC: {perf['main_auc']:.4f}")
            print(f"   Stars:")
            print(f"     - Log-loss: {perf['star_logloss']:.4f} Â± {perf['star_cv_std']:.4f}")
            print(f"     - AUC: {perf['star_auc']:.4f}")
            
            print(f"\nğŸš€ Improvements implemented:")
            for imp in result["improvements"]:
                print(f"   âœ“ {imp}")
            
            print(f"\nğŸ’¾ Models saved to: models/euromillions/improved_*")
        else:
            print(f"\nâŒ Training failed")
    
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
